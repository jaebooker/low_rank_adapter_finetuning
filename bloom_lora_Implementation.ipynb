{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuClass": "premium",
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Simplified LoRA Implementation"
      ],
      "metadata": {
        "id": "D9aAU6JeCQGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Install Dependencies"
      ],
      "metadata": {
        "id": "aqlztwUrCUaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q bitsandbytes datasets accelerate loralib\n",
        "!pip install -q git+https://github.com/huggingface/peft.git git+https://github.com/huggingface/transformers.git\n",
        "!pip install --upgrade peft"
      ],
      "metadata": {
        "id": "R3Gy0KqgByeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Confirm CUDA"
      ],
      "metadata": {
        "id": "O9pwsBMNCWTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "DZAtLKXDB4ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load Base Model"
      ],
      "metadata": {
        "id": "rhn78FE4CZS9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0BZjNgEBvXH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import bitsandbytes as bnb\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"bigscience/bloom-3b\",\n",
        "    device_map='auto',\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/tokenizer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### View Model Summary"
      ],
      "metadata": {
        "id": "Qs2IKgRrCc3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "0ihHmXefB6i9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad = False  # freeze the model - train adapters later\n",
        "  if param.ndim == 1:\n",
        "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
        "    param.data = param.data.to(torch.float32)\n",
        "\n",
        "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "class CastOutputToFloat(nn.Sequential):\n",
        "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
        "model.lm_head = CastOutputToFloat(model.lm_head)"
      ],
      "metadata": {
        "id": "WuK0lPwcB7Ia"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Helper Function"
      ],
      "metadata": {
        "id": "zdlTV1TNCMm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "Cc8354XxCIWl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Obtain LoRA Model"
      ],
      "metadata": {
        "id": "Rc-vqvtuCim3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"query_key_value\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "id": "UQ-cH7ieCARh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load Sample Dataset"
      ],
      "metadata": {
        "id": "iopX35giC7L1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "qa_dataset = load_dataset(\"squad_v2\")"
      ],
      "metadata": {
        "id": "swctuqb8C9YD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "### CONTEXT\n",
        "{context}\n",
        "\n",
        "### QUESTION\n",
        "{question}\n",
        "\n",
        "### ANSWER\n",
        "{answer}</s>\n",
        "```"
      ],
      "metadata": {
        "id": "AD0u5hP3WnXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt(context, question, answer):\n",
        "  if len(answer[\"text\"]) < 1:\n",
        "    answer = \"Cannot Find Answer\"\n",
        "  else:\n",
        "    answer = answer[\"text\"][0]\n",
        "  prompt_template = f\"### CONTEXT\\n{context}\\n\\n### QUESTION\\n{question}\\n\\n### ANSWER\\n{answer}</s>\"\n",
        "  return prompt_template\n",
        "\n",
        "mapped_qa_dataset = qa_dataset.map(lambda samples: tokenizer(create_prompt(samples['context'], samples['question'], samples['answers'])))"
      ],
      "metadata": {
        "id": "3BvDnYjxWPza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train LoRA"
      ],
      "metadata": {
        "id": "fqRzEfHhCoC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=mapped_qa_dataset[\"train\"],\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=100,\n",
        "        max_steps=100,\n",
        "        learning_rate=1e-3,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        output_dir='outputs',\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
      ],
      "metadata": {
        "id": "LNZ3Txn4CFeR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "Vq_pKGde389_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HUGGING_FACE_USER_NAME = \"[insert]\""
      ],
      "metadata": {
        "id": "qYhjq_QFfaTL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "X9cUg-t8fbeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"[insert]\"\n",
        "\n",
        "model.push_to_hub(f\"{HUGGING_FACE_USER_NAME}/{model_name}\", use_auth_token=True)"
      ],
      "metadata": {
        "id": "5_hYow_9fJN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "model_name = \"squidsquad\"\n",
        "peft_model_id = f\"{HUGGING_FACE_USER_NAME}/{model_name}\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=False, device_map='auto')\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "# Load the Lora model\n",
        "qa_model = PeftModel.from_pretrained(model, peft_model_id)"
      ],
      "metadata": {
        "id": "3pqGsRVVfnUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def make_inference(context, question):\n",
        "  batch = tokenizer(f\"### CONTEXT\\n{context}\\n\\n### QUESTION\\n{question}\\n\\n### ANSWER\\n\", return_tensors='pt')\n",
        "\n",
        "  with torch.cuda.amp.autocast():\n",
        "    output_tokens = qa_model.generate(**batch, max_new_tokens=200)\n",
        "\n",
        "  display(Markdown((tokenizer.decode(output_tokens[0], skip_special_tokens=True))))"
      ],
      "metadata": {
        "id": "XAqywI0-ewEK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"Cheese is the best food.\"\n",
        "question = \"What is the best food?\"\n",
        "\n",
        "make_inference(context, question)"
      ],
      "metadata": {
        "id": "c2wMsGB8in7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"Cheese is the best food.\"\n",
        "question = \"How far away is the Moon from the Earth?\"\n",
        "\n",
        "make_inference(context, question)"
      ],
      "metadata": {
        "id": "xmFTFR13iq6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"The Moon orbits Earth at an average distance of 384,400 km (238,900 mi), or about 30 times Earth's diameter. Its gravitational influence is the main driver of Earth's tides and very slowly lengthens Earth's day. The Moon's orbit around Earth has a sidereal period of 27.3 days. During each synodic period of 29.5 days, the amount of visible surface illuminated by the Sun varies from none up to 100%, resulting in lunar phases that form the basis for the months of a lunar calendar. The Moon is tidally locked to Earth, which means that the length of a full rotation of the Moon on its own axis causes its same side (the near side) to always face Earth, and the somewhat longer lunar day is the same as the synodic period. However, 59% of the total lunar surface can be seen from Earth through cyclical shifts in perspective known as libration.\"\n",
        "question = \"At what distance does the Moon orbit the Earth?\"\n",
        "\n",
        "make_inference(context, question)"
      ],
      "metadata": {
        "id": "M3qCyQpxe3s3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "marketmail_model = PeftModel.from_pretrained(model, \"c-s-ale/bloom-7b1-marketmail-ai\")"
      ],
      "metadata": {
        "id": "lJbf0GQMplny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def make_inference_mm_ai(product, description):\n",
        "  batch = tokenizer(f\"Below is a product and description, please write a marketing email for this product.\\n\\n### Product:\\n{product}\\n### Description:\\n{description}\\n\\n### Marketing Email:\\n\", return_tensors='pt')\n",
        "\n",
        "  with torch.cuda.amp.autocast():\n",
        "    output_tokens = marketmail_model.generate(**batch, max_new_tokens=200)\n",
        "\n",
        "  display(Markdown((tokenizer.decode(output_tokens[0], skip_special_tokens=True))))"
      ],
      "metadata": {
        "id": "kfAO01v-qEPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "your_product_name_here = \"The Coolinator\"\n",
        "your_product_description_here = \"A personal cooling device to keep you from getting overheated on a hot summer's day!\"\n",
        "\n",
        "make_inference_mm_ai(your_product_name_here, your_product_description_here)"
      ],
      "metadata": {
        "id": "JuXMXl7-qJCD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}